{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaussian_Process_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "j18U7lBOfkxb",
        "WYyxwzY4omSb",
        "7jyhoxhBTuix",
        "QHks91-m_zte",
        "AnR1Ah4yparo",
        "peztKxh3pGj2",
        "XSpztB5usFL0",
        "gu7enmNwCB4I",
        "Rimm0DSuRfgE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dddonghwa/IAB/blob/main/Gaussian_Process_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXIjxTtaoIm1"
      },
      "source": [
        "# **Gaussian Process Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j18U7lBOfkxb"
      },
      "source": [
        "## **[OVERVIEW]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuVHfWjSkR7f"
      },
      "source": [
        "**Step 1: Define Gaussian RBF** <br>\n",
        "We start by defining the kernel function to be used in our Gaussian Process model. This part should be a quick review from \"Linear Regression\". <br><br>\n",
        "**Step 2: Random Data Generation** <br>\n",
        "We predefine some function as the true function. We then create random samples by adding Gaussian noise. <br><br>\n",
        "**Step 3: Prior Distribution Visualization** <br>\n",
        "We take a look at how the unconditioned Gaussian process model looks like. <br><br>\n",
        "**Step 4: Posterior Distribution Visualization** <br>\n",
        "Given some random samples (training data), we visualize how the conditioned Gaussian process model looks like. <br><br>\n",
        "**Step 5: Optimization of Hyperparameters** <br>\n",
        "We redefine everything done from steps 1 through 4 with PyTorch instead of Numpy. Next, through backpropagation (gradient descent algorithm) we optimize the hyperparameters that define the Gaussian Process model.<br><br>\n",
        "**EXTRA: Easy Implementation with Scikit-Learn**<br>\n",
        "We realize that using scikit-learn's module, steps 1~5 can be implemented fairly easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYyxwzY4omSb"
      },
      "source": [
        "## [STEP 1] Defining the Gaussian radial basis function (Gaussian rbf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cucZPaTwes7F"
      },
      "source": [
        "# Import useful libraries!\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jyhoxhBTuix"
      },
      "source": [
        "### Role of kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdnXwnFqTyoa"
      },
      "source": [
        "x = np.linspace(-10, 10, 1000)\n",
        "y = np.sin(x) + np.random.normal(scale=0.1, size=1000)\n",
        "a = 1\n",
        "y_pred = a * x\n",
        "plt.plot(x, y, '.', label='Data points')\n",
        "plt.plot(x, y_pred, linewidth=4.0, label='Linear regression model')\n",
        "plt.xlim([-10, 10])\n",
        "plt.ylim([-1.1, 1.1])\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih1XVHfT3cF"
      },
      "source": [
        "Define $f(\\textbf{x})=\\sin{\\textbf{x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-vT1rWLT4mT"
      },
      "source": [
        "f_x = np.sin(x)\n",
        "y_pred_with_f = a * f_x\n",
        "plt.plot(f_x, y, '.', label='Data points')\n",
        "plt.plot(f_x, y_pred_with_f, linewidth=4.0, label='(General) Linear regression model')\n",
        "plt.xlabel('sin(x)')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaTJtaOVT_JV"
      },
      "source": [
        "Define $\\phi(\\textbf{x})=\\sigma^2_f\\exp{\\left(-\\frac{1}{2l^2}\\left(\\textbf{x}-\\mu\\right)^2\\right)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwbWCGCZUAcN"
      },
      "source": [
        "mu1 = 0\n",
        "mu2 = -5\n",
        "mu3 = 5\n",
        "sigma_f_squared = 1.0\n",
        "l_squared = 0.1\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "phi1 = sigma_f_squared * np.exp(-np.square(x-mu1) / (2*l_squared))\n",
        "phi2 = sigma_f_squared * np.exp(-np.square(x-mu2) / (2*l_squared))\n",
        "phi3 = sigma_f_squared * np.exp(-np.square(x-mu3) / (2*l_squared))\n",
        "plt.plot(x, phi1, label='mu={}'.format(mu1))\n",
        "plt.plot(x, phi2, label='mu={}'.format(mu2))\n",
        "plt.plot(x, phi3, label='mu={}'.format(mu3))\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim([-10, 10])\n",
        "plt.ylim([-1, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHks91-m_zte"
      },
      "source": [
        "### Gaussian RBF\n",
        "Radial basis function for two vectors of same dimension $\\textbf{x}_i, \\textbf{x}_j\\in\\mathbb{R}^d$\n",
        "\\begin{equation}\n",
        "K(\\textbf{x}_i, \\textbf{x}_j) = \n",
        "\\sigma^2_f \\exp{\\left( -\\frac{1} {2l^2}(\\textbf{x}_i-\\textbf{x}_j)^T(\\textbf{x}_i-\\textbf{x}_j)\\right)} \\\\\n",
        "= \n",
        "\\sigma^2_f \\exp{\\left( -\\frac{1} {2l^2}(\\textbf{x}_i^T\\textbf{x}_i-2\\textbf{x}_i^T\\textbf{x}_j+\\textbf{x}_j^T\\textbf{x}_j)\\right)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKUzBYTT--5q"
      },
      "source": [
        "# Kernel definitions\n",
        "def vanilla_gaussian_rbf(x1, x2, l=1, sigma_f=1):\n",
        "    assert x1.shape[1] == 1 and x2.shape[1] == 1\n",
        "    xiTxi = np.dot(x1.T, x1).reshape(1)\n",
        "    xiTxj = np.dot(x1.T, x2).reshape(1)\n",
        "    xjTxj = np.dot(x2.T, x2).reshape(1)\n",
        "    dist_matrix = xiTxi - 2*xiTxj + xjTxj\n",
        "    return (sigma_f**2) * np.exp((-1/(l**2)) * dist_matrix)\n",
        "\n",
        "# TEST\n",
        "x1 = np.zeros((10, 1))\n",
        "x2 = np.zeros((10, 1))\n",
        "out = vanilla_gaussian_rbf(x1, x2)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1bnFzFmNIUc"
      },
      "source": [
        "Radial basis function between two matrices $\\textbf{X}\\in\\mathbb{R}^{m\\times d}, \\textbf{Y}\\in\\mathbb{R}^{n \\times d}$\n",
        "\\begin{equation}\n",
        "K(\\textbf{X}, \\textbf{Y}) = \n",
        "\\begin{bmatrix}\n",
        "K(\\textbf{x}_1, \\textbf{y}_1) & K(\\textbf{x}_1, \\textbf{y}_2) & \\cdots & K(\\textbf{x}_1, \\textbf{y}_n)\\\\\n",
        "K(\\textbf{x}_2, \\textbf{y}_1) & K(\\textbf{x}_2, \\textbf{y}_2) & \\cdots & K(\\textbf{x}_2, \\textbf{y}_n)\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "K(\\textbf{x}_m, \\textbf{y}_1) & K(\\textbf{x}_m, \\textbf{y}_2) & \\cdots & K(\\textbf{x}_m, \\textbf{y}_n)\n",
        "\\end{bmatrix} \\\\\n",
        "= \\sigma_f^2\\exp{\\left(-\\frac{1}{2l^2}\n",
        "\\begin{bmatrix}\n",
        "(\\textbf{x}_1^T\\textbf{x}_1-2\\textbf{x}_1^T\\textbf{y}_1+\\textbf{y}_1^T\\textbf{y}_1) & (\\textbf{x}_1^T\\textbf{x}_1-2\\textbf{x}_1^T\\textbf{y}_2+\\textbf{y}_2^T\\textbf{y}_2) & \\cdots & (\\textbf{x}_1^T\\textbf{x}_1-2\\textbf{x}_1^T\\textbf{y}_n+\\textbf{y}_n^T\\textbf{y}_n)\\\\\n",
        "(\\textbf{x}_2^T\\textbf{x}_2-2\\textbf{x}_2^T\\textbf{y}_1+\\textbf{y}_1^T\\textbf{y}_1) & (\\textbf{x}_2^T\\textbf{x}_2-2\\textbf{x}_2^T\\textbf{y}_2+\\textbf{y}_2^T\\textbf{y}_2) & \\cdots & (\\textbf{x}_2^T\\textbf{x}_2-2\\textbf{x}_2^T\\textbf{y}_n+\\textbf{y}_n^T\\textbf{y}_n)\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "(\\textbf{x}_m^T\\textbf{x}_m-2\\textbf{x}_m^T\\textbf{y}_1+\\textbf{y}_1^T\\textbf{y}_1) & (\\textbf{x}_m^T\\textbf{x}_m-2\\textbf{x}_m^T\\textbf{y}_2+\\textbf{y}_2^T\\textbf{y}_2) & \\cdots & (\\textbf{x}_m^T\\textbf{x}_m-2\\textbf{x}_m^T\\textbf{y}_n+\\textbf{y}_n^T\\textbf{y}_n)\n",
        "\\end{bmatrix}\n",
        "\\right)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkMOS4voicAh"
      },
      "source": [
        "NOTE: Distance matrix can be computed as follows using Python's (Numpy's) broadcasting method!\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\\textbf{x}_1^T\\textbf{x}_1 \\\\\n",
        "\\textbf{x}_2^T\\textbf{x}_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\textbf{x}_m^T\\textbf{x}_m\n",
        "\\end{bmatrix}\n",
        "-2\n",
        "\\begin{bmatrix}\n",
        "\\textbf{x}_1^T\\textbf{y}_1 & \\textbf{x}_1^T\\textbf{y}_2 & \\cdots & \\textbf{x}_1^T\\textbf{y}_n \\\\\n",
        "\\textbf{x}_2^T\\textbf{y}_1 & \\textbf{x}_2^T\\textbf{y}_2 & \\cdots & \\textbf{x}_2^T\\textbf{y}_n \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\textbf{x}_m^T\\textbf{y}_1 & \\textbf{x}_m^T\\textbf{y}_2 & \\cdots & \\textbf{x}_m^T\\textbf{y}_n\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "\\textbf{y}_1^T\\textbf{y}_1 & \\textbf{y}_2^T\\textbf{y}_2 & \\cdots & \\textbf{y}_n^T\\textbf{y}_n\n",
        "\\end{bmatrix}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7-gD_X9Fc08"
      },
      "source": [
        "def gaussian_rbf(X, Y, l=1, sigma_f=1):\n",
        "    assert X.shape[1] == Y.shape[1] # dimension must be equal!\n",
        "    m, d = X.shape\n",
        "    n, d = Y.shape\n",
        "    term1 = np.sum(X**2, axis=1).reshape(-1, 1)\n",
        "    assert term1.shape == (m, 1)\n",
        "    term2 = np.dot(X, Y.T)\n",
        "    assert term2.shape == (m, n)\n",
        "    term3 = np.sum(Y**2, axis=1).reshape(1, -1)\n",
        "    assert term3.shape == (1, n)\n",
        "    dist_matrix = term1 - 2*term2 + term3\n",
        "    return (sigma_f**2) * np.exp((-1/(l**2)) * dist_matrix)\n",
        "\n",
        "# TEST\n",
        "X = np.zeros((5, 5))\n",
        "Y = np.zeros((5, 5))\n",
        "out = gaussian_rbf(X, Y)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnR1Ah4yparo"
      },
      "source": [
        "## [STEP 2] Random Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Teh1w5pgbM"
      },
      "source": [
        "Suppose true data is defined as follows: \n",
        "\\begin{equation}\n",
        "f(x) = \\sin(x)+0.05x^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeeidqFnpfRT"
      },
      "source": [
        "def true_function(x):\n",
        "    return np.sin(x) + 0.05 * (x ** 2)\n",
        "\n",
        "# True function\n",
        "x_true = np.linspace(-2*np.pi, 2*np.pi, 100) # 100 data points between -2pi and 2pi\n",
        "y_true = true_function(x_true)\n",
        "y_mean = np.mean(y_true)\n",
        "y_true -= y_mean # We are assuming an expected value of 0 from our prior distribution for simplicity, so just offset all values by the mean value\n",
        "\n",
        "# Sample data\n",
        "n = 20\n",
        "x_data = np.linspace(-1.5*np.pi, 1.5*np.pi, n)\n",
        "random_noise = 0.3 * np.random.randn(n) # assume sampled data has some noise\n",
        "y_data = true_function(x_data) + random_noise\n",
        "y_data -= y_mean # offset by same mean from real data!\n",
        "\n",
        "# Plot true function with sampled data points\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x_true, y_true, 'k-', label='True function')\n",
        "plt.plot(x_data, y_data, 'r.', label='Sample data')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim([-2*np.pi, 2*np.pi])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peztKxh3pGj2"
      },
      "source": [
        "## [STEP 3] Prior Distribution Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8zJnioImkMI"
      },
      "source": [
        "Prior distribution (before learning from data points) given as follows:\n",
        "\\begin{equation}\n",
        "\\textbf{f}\\sim\\mathcal{N}(0, K(\\textbf{X}, \\textbf{X}))\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8YCFS8aTDxy"
      },
      "source": [
        "# Prior distribution\n",
        "X = x_true.reshape(-1, 1)\n",
        "mu = np.zeros(X.shape[0])\n",
        "cov = gaussian_rbf(X, X)\n",
        "samples = np.random.multivariate_normal(mu.reshape(-1), cov, 10)\n",
        "\n",
        "def plot_gp(mu, cov, X, samples=[]):\n",
        "    # Assumes data points are 1-dimensional!\n",
        "    X = X.reshape(-1)\n",
        "    mu = mu.reshape(-1)\n",
        "    # 95% confidence interval\n",
        "    uncertainty = 1.96 * np.sqrt(np.diag(cov))\n",
        "    plt.fill_between(X, mu+uncertainty, mu-uncertainty, alpha=0.1)\n",
        "    plt.plot(X, mu, label='mean')\n",
        "    for i, sample in enumerate(samples):\n",
        "        plt.plot(X, sample, lw=1, ls='--', label='sample_{}'.format(i))\n",
        "    plt.legend()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_gp(mu, cov, X, samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpztB5usFL0"
      },
      "source": [
        "## [STEP 4] Posterior Distribution Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_glNlCyIsHVD"
      },
      "source": [
        "Given sampled data points, we can obtain the posterior distribution as follows:\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\\textbf{f} \\\\ \\textbf{f}_*\n",
        "\\end{bmatrix}\n",
        "\\sim\\mathcal{N}\\left(0, \n",
        "\\begin{pmatrix}\n",
        "K(\\textbf{X}, \\textbf{X}) & K(\\textbf{X}, \\textbf{X}_*) \\\\\n",
        "K(\\textbf{X}_*, \\textbf{X}) & K(\\textbf{X}_*, \\textbf{X}_*)\n",
        "\\end{pmatrix}\n",
        "\\right)\n",
        "\\end{equation}\n",
        "For simplicity let's denote the matrices as follows:\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\\textbf{f} \\\\ \\textbf{f}_*\n",
        "\\end{bmatrix}\n",
        "\\sim\\mathcal{N}\\left(0, \n",
        "\\begin{pmatrix}\n",
        "K_{11} & K_{12} \\\\\n",
        "K_{21} & K_{22}\n",
        "\\end{pmatrix}\n",
        "\\right)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQI_YfZWtYFU"
      },
      "source": [
        "Then, the conditional distribution of these jointly Gaussian random vectors is given as follows:\n",
        "\\begin{equation}\n",
        "\\textbf{f}_* \\mid \\textbf{X}_*, \\textbf{X}, \\textbf{f} \\sim\\mathcal{N}\\left(\n",
        "    K_{21}K_{11}^{-1}\\textbf{f} \\quad, \\quad \n",
        "    K_{22}-K_{21}K_{11}^{-1}K_{12}\\right)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlhkpRHh-f5e"
      },
      "source": [
        "If we assume that sampled data points are noisy, i.e., \n",
        "\\begin{equation}\n",
        "y(x)=f(x) + \\epsilon \\quad, \\quad \\epsilon\\sim\\mathcal{N}(0, \\sigma_y^2)\n",
        "\\end{equation}\n",
        "Then, the joint Gaussian can be represented as follows:\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\\textbf{y} \\\\ \\textbf{f}_*\n",
        "\\end{bmatrix}\n",
        "\\sim\\mathcal{N}\\left(0, \n",
        "\\begin{pmatrix}\n",
        "K(\\textbf{X}, \\textbf{X})+\\sigma_y^2\\mathbb{I} & K(\\textbf{X}, \\textbf{X}_*) \\\\\n",
        "K(\\textbf{X}_*, \\textbf{X}) & K(\\textbf{X}_*, \\textbf{X}_*)\n",
        "\\end{pmatrix}\n",
        "\\right)\n",
        "\\end{equation}\n",
        "And thus, the conditional distribution is as follows:\n",
        "\\begin{equation}\n",
        "\\textbf{f}_* \\mid \\textbf{X}_*, \\textbf{X}, \\textbf{y} \\sim\\mathcal{N}\\left(\n",
        "    K_{21}K_{11}^{-1}\\textbf{y} \\quad, \\quad \n",
        "    K_{22}-K_{21}K_{11}^{-1}K_{12}\\right)\n",
        "\\end{equation}\n",
        "with\n",
        "\\begin{equation}\n",
        "K_{11} = K(\\textbf{X}, \\textbf{X}) + \\sigma_y^2\\mathbb{I}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MwjyFJndvXd"
      },
      "source": [
        "# Posterior distribution\n",
        "def predict_posterior(X, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
        "    ###### TO IMPLEMENT ######\n",
        "    K11 = \n",
        "    K21 = \n",
        "    K12 = \n",
        "    K22 = \n",
        "    K11_inv = np.linalg.inv(K11)\n",
        "    mu = \n",
        "    cov = \n",
        "    ##########################\n",
        "    return mu, cov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR3UecnVegPM"
      },
      "source": [
        "# Posterior distribution's mu and cov\n",
        "mu, cov = predict_posterior(X=x_true.reshape(-1, 1), \n",
        "                            X_train=x_data.reshape(-1, 1), \n",
        "                            Y_train=y_data.reshape(-1, 1), \n",
        "                            l=1.0, \n",
        "                            sigma_f=1.0, \n",
        "                            sigma_y=1.0)\n",
        "samples = np.random.multivariate_normal(mu.reshape(-1), cov, 0)\n",
        "\n",
        "# Plot true function and sampled data points\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x_true, y_true, 'k-', label='True function')\n",
        "plt.plot(x_data, y_data, 'r.', label='Sample data')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim([-2*np.pi, 2*np.pi])\n",
        "\n",
        "# Plot predictions from learning\n",
        "plot_gp(mu, cov, x_true, samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu7enmNwCB4I"
      },
      "source": [
        "## [STEP 5] Learning the Optimal Hyperparameter Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ajjAPZFDFya"
      },
      "source": [
        "Note that the parameters used, $l, \\sigma_f^2, \\sigma_y^2$, do not have the optimal values! <br>\n",
        "We can obtain the best parameters by maximizing the log-likelihood:\n",
        "\\begin{equation}\n",
        "\\textbf{y}\\sim\\mathcal{N}(0, K+\\sigma_y^2\\mathbb{I}) \\\\\n",
        "\\log P(y\\mid\\textbf{X}) = \n",
        "-\\frac{1}{2}\\textbf{y}^T(K+\\sigma_y^2\\mathbb{I})^{-1}\\textbf{y}\n",
        "-\\frac{1}{2}\\log|K+\\sigma_y^2\\mathbb{I}|-\\frac{n}{2}\\log 2\\pi\n",
        "\\end{equation}\n",
        "Note that maximizing the above log-likelihood is equivalent to minimizing the following negative log-likelihood (usually denoted as NLL):\n",
        "\\begin{equation}\n",
        "NLL = \\frac{1}{2}\\textbf{y}^T(K+\\sigma_y^2\\mathbb{I})^{-1}\\textbf{y}\n",
        "+\\frac{1}{2}\\log|K+\\sigma_y^2\\mathbb{I}|\n",
        "\\end{equation}\n",
        "In order to facilitate this optimization process, we will create a class for the Gaussian process model, and using PyTorch's module, optimize via the gradient descent method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyRlog5IJ6Q1"
      },
      "source": [
        "class GPR():\n",
        "    def __init__(self):\n",
        "        self.l = torch.tensor(1e1, requires_grad=True)\n",
        "        self.sigma_f = torch.tensor(1e0, requires_grad=True)\n",
        "        self.sigma_y = torch.tensor(1e-4, requires_grad=True)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.parameters = [self.l, self.sigma_f, self.sigma_y]\n",
        "        self.optimizer = torch.optim.Adam(self.parameters, lr=1e-3)\n",
        "\n",
        "    def GaussianRBF(self, X, Y):\n",
        "        assert X.shape[1] == Y.shape[1] # dimension must be equal!\n",
        "        m, d = X.shape\n",
        "        n, d = Y.shape\n",
        "        term1 = torch.sum(X**2, dim=1).view(-1, 1)\n",
        "        assert term1.shape == (m, 1)\n",
        "        term2 = torch.matmul(X, Y.t())\n",
        "        assert term2.shape == (m, n)\n",
        "        term3 = torch.sum(Y**2, dim=1).view(1, -1)\n",
        "        assert term3.shape == (1, n)\n",
        "        dist_matrix = term1 - 2*term2 + term3\n",
        "        return (self.sigma_f**2) * torch.exp((-1/(self.l**2)) * dist_matrix)\n",
        "\n",
        "    def predict_posterior(self, X, X_train, Y_train):\n",
        "        K11 = self.GaussianRBF(X_train, X_train) + self.sigma_y**2 * torch.eye(len(X_train))\n",
        "        K21 = self.GaussianRBF(X, X_train)\n",
        "        K12 = self.GaussianRBF(X_train, X) # Note, K12 = K21.T (transposed)\n",
        "        K22 = self.GaussianRBF(X, X)\n",
        "        K11_inv = torch.linalg.inv(K11)\n",
        "        mu = torch.matmul(K21, torch.matmul(K11_inv, Y_train))\n",
        "        cov = K22 - torch.matmul(K21, torch.matmul(K11_inv, K12))\n",
        "        return mu, cov\n",
        "\n",
        "    def set_hyperparameter(self, l, sigma_f, sigma_y):\n",
        "        self.l = torch.tensor(l, requires_grad=True)\n",
        "        self.sigma_f = torch.tensor(sigma_f, requires_grad=True)\n",
        "        self.sigma_y = torch.tensor(sigma_y, requires_grad=True)\n",
        "        self.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpYX2DbPNUEu"
      },
      "source": [
        "# Sanity check!\n",
        "\n",
        "gpr = GPR()\n",
        "l, sigma_f, sigma_y = 1.0, 1.0, 1.0\n",
        "gpr.set_hyperparameter(l, sigma_f, sigma_y)\n",
        "X_tensor = torch.from_numpy(x_true.reshape(-1, 1))\n",
        "X_train_tensor = torch.from_numpy(x_data.reshape(-1, 1))\n",
        "Y_train_tensor = torch.from_numpy(y_data.reshape(-1, 1))\n",
        "mu, cov = gpr.predict_posterior(X_tensor, X_train_tensor, Y_train_tensor)\n",
        "mu = mu.detach().numpy()\n",
        "cov = cov.detach().numpy()\n",
        "samples = np.random.multivariate_normal(mu.reshape(-1), cov, 3)\n",
        "\n",
        "# Plot true function and sampled data points\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x_true, y_true, 'k-', label='True function')\n",
        "plt.plot(x_data, y_data, 'r.', label='Sample data')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim([-2*np.pi, 2*np.pi])\n",
        "\n",
        "# Plot predictions from learning\n",
        "plot_gp(mu, cov, x_true, samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J0WKUfK6CA7"
      },
      "source": [
        "Recall, we need to minimize the following:\n",
        "\\begin{equation}\n",
        "NLL = \\frac{1}{2}\\textbf{y}^T(K+\\sigma_y^2\\mathbb{I})^{-1}\\textbf{y}\n",
        "+\\frac{1}{2}\\log|K+\\sigma_y^2\\mathbb{I}|\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y3HadM6OypL"
      },
      "source": [
        "# Optimize parameters\n",
        "gpr.set_hyperparameter(1.0, 1.0, 1.0)\n",
        "X_tensor = torch.from_numpy(x_true.reshape(-1, 1))\n",
        "X_train_tensor = torch.from_numpy(x_data.reshape(-1, 1))\n",
        "Y_train_tensor = torch.from_numpy(y_data.reshape(-1, 1))\n",
        "\n",
        "for i in range(5000):\n",
        "    K = gpr.GaussianRBF(X_train_tensor, X_train_tensor)\n",
        "    K = K + ((gpr.sigma_y+1e-10)**2) * torch.eye(len(X_train_tensor))\n",
        "    ###### TO IMPLEMENT ######\n",
        "    loss = \n",
        "    ##########################\n",
        "    gpr.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    gpr.optimizer.step()\n",
        "    if (i+1)%200 == 0:\n",
        "        print('Iter[{}/5000]\\tloss:{:.4f}\\tl:{:.4f}\\tsigma_f:{:.4f}\\tsigma_y:{:.4f}'.format(i+1, loss.item(), gpr.l.item(), gpr.sigma_f.item(), gpr.sigma_y.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJUW7xuLRAo5"
      },
      "source": [
        "# Plot with optimized model!\n",
        "mu, cov = gpr.predict_posterior(X_tensor, X_train_tensor, Y_train_tensor)\n",
        "mu = mu.detach().numpy()\n",
        "cov = cov.detach().numpy()\n",
        "samples = np.random.multivariate_normal(mu.reshape(-1), cov, 0)\n",
        "\n",
        "# Plot true function and sampled data points\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x_true, y_true, 'k-', label='True function')\n",
        "plt.plot(x_data, y_data, 'r.', label='Sample data')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim([-2*np.pi, 2*np.pi])\n",
        "\n",
        "# Plot predictions from learning\n",
        "plot_gp(mu, cov, x_true, samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rimm0DSuRfgE"
      },
      "source": [
        "## [EXTRA] Easy implementation using scikit-learn package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx9QlzM0gGNw"
      },
      "source": [
        "# Easy implementation with scikit-learn package\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
        "\n",
        "# Set initial hyperparameters\n",
        "init_lambda = 10. # l\n",
        "init_beta = 1. # sigma_f\n",
        "init_sigma = 0.04 # sigma_y\n",
        "\n",
        "# Initialize GaussianRBF kernel and GPR model\n",
        "kernel = ConstantKernel(init_beta, (1e-3, 1e3)) * RBF(init_lambda, (1e-3, 1e3))\n",
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=init_sigma, n_restarts_optimizer=9)\n",
        "\n",
        "# Reshape arrays into 2d arrays\n",
        "x_gp = x_true.reshape(-1, 1)\n",
        "y_gp = y_true.reshape(-1, 1)\n",
        "X_train_gp = x_data.reshape(-1, 1)\n",
        "Y_train_gp = y_data.reshape(-1, 1)\n",
        "\n",
        "# Optimize parameters and obtain results\n",
        "gp.fit(X_train_gp, Y_train_gp)\n",
        "Y_pred_sk, std_pred_sk = gp.predict(x_gp, return_std=True)\n",
        "Y_pred_sk = Y_pred_sk.flatten()\n",
        "std_pred_sk = std_pred_sk.flatten()\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(x_true, y_true, 'k-', label='True function')\n",
        "plt.plot(x_data, y_data, 'r.', label='Sample data')\n",
        "plt.plot(x_true, Y_pred_sk, 'b-', label='GPR')\n",
        "plt.fill_between(x_true, Y_pred_sk-1.96*std_pred_sk, Y_pred_sk+1.96*std_pred_sk, color='grey', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlim([-2*np.pi, 2*np.pi])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
